# Guide des Techniques NLP : De la Fr√©quence au Sens

Ce guide d√©finit les quatre piliers fondamentaux du traitement du langage naturel (NLP). Pour chaque concept, nous explorons son r√¥le, sa justification (**Pourquoi**), son fonctionnement interne (**Comment**) et ses cas d'usage r√©els (**O√π**).

---

## 1. Bag of Words (BoW) - Le Sac de Mots
**"Le Compteur Simple"**

### üéØ R√¥le
C'est la technique la plus ancienne et la plus simple. Elle transforme un texte en un vecteur de nombres en comptant simplement la fr√©quence d'apparition de chaque mot, sans se soucier de l'ordre ou de la grammaire.

### ‚ùì Pourquoi l'utiliser ?
* **Simplicit√© extr√™me :** Tr√®s facile √† comprendre et √† impl√©menter.
* **Rapidit√© :** N√©cessite peu de puissance de calcul.
* **Efficacit√© sur des t√¢ches simples :** Parfait si la pr√©sence d'un mot sp√©cifique (ex: "gagnant", "gratuit") suffit √† classer le texte.

### ‚öôÔ∏è Comment √ßa marche ?
1.  **Vocabulaire :** On liste tous les mots uniques de tout le corpus (ex: 10 000 mots).
2.  **Vectorisation :** Pour chaque phrase, on cr√©e un vecteur de la taille du vocabulaire.
3.  **Comptage :** Si le mot "chat" est le 3√®me mot du vocabulaire et appara√Æt 2 fois dans la phrase, la 3√®me position du vecteur prend la valeur `2`. Ailleurs, c'est `0`.

### üìç O√π l'utiliser ?
* **Filtrage de Spam :** D√©tecter des mots cl√©s suspects.
* **Classification de documents simple :** Classer des articles par th√®mes (Sport, Politique) bas√©s sur des mots cl√©s √©vidents.

---

## 2. TF-IDF (Term Frequency - Inverse Document Frequency)
**"Le Sp√©cialiste"**

### üéØ R√¥le
Une am√©lioration du BoW. Au lieu de compter b√™tement, TF-IDF donne du poids aux mots **importants** et p√©nalise les mots trop courants (comme "le", "de", "est") qui n'apportent pas d'information.

### ‚ùì Pourquoi l'utiliser ?
* **Filtrage du bruit :** Dans BoW, le mot "le" appara√Æt 100 fois et semble important. TF-IDF r√©duit son score √† pr√®s de z√©ro.
* **Mise en valeur de la raret√© :** Un mot rare qui appara√Æt dans un document sp√©cifique devient la "signature" de ce document.

### ‚öôÔ∏è Comment √ßa marche ?
C'est une multiplication de deux scores :
1.  **TF (Fr√©quence du terme) :** Combien de fois le mot appara√Æt dans *ce* document (comme BoW).
2.  **IDF (Fr√©quence inverse de document) :** Un calcul logarithmique qui diminue le score si le mot appara√Æt dans *beaucoup* de documents du corpus.

### üìç O√π l'utiliser ?
* **Moteurs de recherche (classiques) :** Pour trouver le document le plus pertinent pour une requ√™te utilisateur.
* **Extraction de mots-cl√©s :** R√©sumer automatiquement le sujet d'un texte.
* **Syst√®mes de recommandation basiques :** "Vous avez aim√© cet article sur le 'Jardinage', voici un autre article avec un score TF-IDF √©lev√© sur 'Jardinage'."

---

## 3. Word2Vec (Word Embeddings)
**"Le S√©mantique Statique"**

### üéØ R√¥le
Word2Vec abandonne le comptage pour l'apprentissage. Il transforme chaque mot en un **vecteur dense** (une liste de coordonn√©es, ex: `[0.2, -0.5, 0.9]`) dans un espace g√©om√©trique. Les mots ayant un sens proche sont g√©ographiquement proches.

### ‚ùì Pourquoi l'utiliser ?
* **Capture du sens (S√©mantique) :** Contrairement √† BoW/TF-IDF, il comprend que "Roi" et "Reine" sont li√©s.
* **Analogies math√©matiques :** On peut faire des calculs sur les mots : `Vecteur(Roi) - Vecteur(Homme) + Vecteur(Femme) ‚âà Vecteur(Reine)`.
* **R√©duction de dimension :** On passe de vecteurs immenses et vides (taille 100 000 avec plein de z√©ros) √† des vecteurs compacts et pleins (taille 300).

### ‚öôÔ∏è Comment √ßa marche ?
Un r√©seau de neurones est entra√Æn√© sur des milliards de phrases pour pr√©dire un mot en fonction de ses voisins (contexte).
* Si "chat" et "chien" apparaissent souvent entour√©s des m√™mes mots ("manger", "dormir", "animal"), le r√©seau apprend √† leur donner des coordonn√©es similaires.
* **Note :** C'est un embedding *statique*. Le mot "avocat" aura le m√™me vecteur, qu'on parle du fruit ou du m√©tier.

### üìç O√π l'utiliser ?
* **Syst√®mes de recommandation avanc√©s :** Sugg√©rer des produits similaires (s√©mantiquement proches).
* **Recherche de synonymes :** Trouver des mots alternatifs.
* **Pr√©-traitement pour Deep Learning :** Nourrir des mod√®les plus complexes (LSTM, CNN).

---

## 4. BERT (Contextual Embeddings)
**"L'Intelligence Contextuelle"**

### üéØ R√¥le
L'√©tat de l'art actuel (bas√© sur les Transformers). Contrairement √† Word2Vec, BERT g√©n√®re des vecteurs qui changent selon le **contexte** de la phrase. Il ne lit pas mot par mot, mais analyse toute la phrase d'un coup.

### ‚ùì Pourquoi l'utiliser ?
* **Polys√©mie (Mots √† double sens) :**
    * Phrase A : "Je mange un **avocat**."
    * Phrase B : "J'ai appel√© mon **avocat**."
    * *Word2Vec* donne le m√™me vecteur pour "avocat". *BERT* donne deux vecteurs totalement diff√©rents car il comprend le contexte.
* **Compr√©hension profonde :** Il saisit la n√©gation, l'ironie et les relations complexes entre les mots.

### ‚öôÔ∏è Comment √ßa marche ?
Il utilise le m√©canisme d'**Attention**. Le mod√®le regarde chaque mot et calcule son lien avec *tous* les autres mots de la phrase simultan√©ment. Il est pr√©-entra√Æn√© sur le web entier (Wikipedia, Livres) en jouant √† des jeux de "texte √† trous" (Masked Language Modeling).

### üìç O√π l'utiliser ?
* **Moteurs de recherche modernes (Google) :** Pour comprendre l'intention derri√®re une requ√™te complexe ("changer pneu voiture prix").
* **Chatbots & Assistants (IA G√©n√©rative) :** Comprendre et r√©pondre humainement.
* **Analyse de sentiments complexe :** Distinguer "Pas mal du tout" (positif) de "Pas mal, mais..." (mitig√©).
* **Traduction automatique.**

---

## R√©sum√© Comparatif

| Technique | Type | Comprend le sens ? | Comprend le contexte ? | Complexit√© |
| :--- | :--- | :--- | :--- | :--- |
| **Bag of Words** | Comptage | Non | Non | Tr√®s Faible |
| **TF-IDF** | Pond√©ration | Non (mais filtre le bruit) | Non | Faible |
| **Word2Vec** | Embedding Statique | **Oui** (mots isol√©s) | Non (1 mot = 1 vecteur) | Moyenne |
| **BERT** | Embedding Dynamique | **Oui** (phrase enti√®re) | **Oui** (s'adapte au contexte) | √âlev√©e (GPU requis) |
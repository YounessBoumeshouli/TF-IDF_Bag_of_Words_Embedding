# Guide des Techniques NLP : De la FrÃ©quence au Sens

Ce guide dÃ©finit les quatre piliers fondamentaux du traitement du langage naturel (NLP). Pour chaque concept, nous explorons son rÃ´le, sa justification (**Pourquoi**), son fonctionnement interne (**Comment**) et ses cas d'usage rÃ©els (**OÃ¹**).

---

## 1. Bag of Words (BoW) - Le Sac de Mots
**"Le Compteur Simple"**

### ğŸ¯ RÃ´le
C'est la technique la plus ancienne et la plus simple. Elle transforme un texte en un vecteur de nombres en comptant simplement la frÃ©quence d'apparition de chaque mot, sans se soucier de l'ordre ou de la grammaire.

### â“ Pourquoi l'utiliser ?
* **SimplicitÃ© extrÃªme :** TrÃ¨s facile Ã  comprendre et Ã  implÃ©menter.
* **RapiditÃ© :** NÃ©cessite peu de puissance de calcul.
* **EfficacitÃ© sur des tÃ¢ches simples :** Parfait si la prÃ©sence d'un mot spÃ©cifique (ex: "gagnant", "gratuit") suffit Ã  classer le texte.

### âš™ï¸ Comment Ã§a marche ?
1.  **Vocabulaire :** On liste tous les mots uniques de tout le corpus (ex: 10 000 mots).
2.  **Vectorisation :** Pour chaque phrase, on crÃ©e un vecteur de la taille du vocabulaire.
3.  **Comptage :** Si le mot "chat" est le 3Ã¨me mot du vocabulaire et apparaÃ®t 2 fois dans la phrase, la 3Ã¨me position du vecteur prend la valeur `2`. Ailleurs, c'est `0`.

### ğŸ“ OÃ¹ l'utiliser ?
* **Filtrage de Spam :** DÃ©tecter des mots clÃ©s suspects.
* **Classification de documents simple :** Classer des articles par thÃ¨mes (Sport, Politique) basÃ©s sur des mots clÃ©s Ã©vidents.

---

## 2. TF-IDF (Term Frequency - Inverse Document Frequency)
**"Le SpÃ©cialiste"**

### ğŸ¯ RÃ´le
Une amÃ©lioration du BoW. Au lieu de compter bÃªtement, TF-IDF donne du poids aux mots **importants** et pÃ©nalise les mots trop courants (comme "le", "de", "est") qui n'apportent pas d'information.

### â“ Pourquoi l'utiliser ?
* **Filtrage du bruit :** Dans BoW, le mot "le" apparaÃ®t 100 fois et semble important. TF-IDF rÃ©duit son score Ã  prÃ¨s de zÃ©ro.
* **Mise en valeur de la raretÃ© :** Un mot rare qui apparaÃ®t dans un document spÃ©cifique devient la "signature" de ce document.

### âš™ï¸ Comment Ã§a marche ?
C'est une multiplication de deux scores :
1.  **TF (FrÃ©quence du terme) :** Combien de fois le mot apparaÃ®t dans *ce* document (comme BoW).
2.  **IDF (FrÃ©quence inverse de document) :** Un calcul logarithmique qui diminue le score si le mot apparaÃ®t dans *beaucoup* de documents du corpus.

### ğŸ“ OÃ¹ l'utiliser ?
* **Moteurs de recherche (classiques) :** Pour trouver le document le plus pertinent pour une requÃªte utilisateur.
* **Extraction de mots-clÃ©s :** RÃ©sumer automatiquement le sujet d'un texte.
* **SystÃ¨mes de recommandation basiques :** "Vous avez aimÃ© cet article sur le 'Jardinage', voici un autre article avec un score TF-IDF Ã©levÃ© sur 'Jardinage'."

---

## 3. Word2Vec (Word Embeddings)
**"Le SÃ©mantique Statique"**

### ğŸ¯ RÃ´le
Word2Vec abandonne le comptage pour l'apprentissage. Il transforme chaque mot en un **vecteur dense** (une liste de coordonnÃ©es, ex: `[0.2, -0.5, 0.9]`) dans un espace gÃ©omÃ©trique. Les mots ayant un sens proche sont gÃ©ographiquement proches.

### â“ Pourquoi l'utiliser ?
* **Capture du sens (SÃ©mantique) :** Contrairement Ã  BoW/TF-IDF, il comprend que "Roi" et "Reine" sont liÃ©s.
* **Analogies mathÃ©matiques :** On peut faire des calculs sur les mots : `Vecteur(Roi) - Vecteur(Homme) + Vecteur(Femme) â‰ˆ Vecteur(Reine)`.
* **RÃ©duction de dimension :** On passe de vecteurs immenses et vides (taille 100 000 avec plein de zÃ©ros) Ã  des vecteurs compacts et pleins (taille 300).

### âš™ï¸ Comment Ã§a marche ?
Un rÃ©seau de neurones est entraÃ®nÃ© sur des milliards de phrases pour prÃ©dire un mot en fonction de ses voisins (contexte).
* Si "chat" et "chien" apparaissent souvent entourÃ©s des mÃªmes mots ("manger", "dormir", "animal"), le rÃ©seau apprend Ã  leur donner des coordonnÃ©es similaires.
* **Note :** C'est un embedding *statique*. Le mot "avocat" aura le mÃªme vecteur, qu'on parle du fruit ou du mÃ©tier.

### ğŸ“ OÃ¹ l'utiliser ?
*